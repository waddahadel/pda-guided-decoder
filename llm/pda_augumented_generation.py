import json
from llm.generate_candidates import get_top_k_candidates
from pda.json_pda import JsonPDA

def generate_with_pda(
    prompt: str,
    model,
    tokenizer,
    max_steps: int = 100,
    top_k: int = 10,
) -> str:
    """
    Generate syntactically valid JSON instances using PDA-guided decoding.
    Focuses only on generating the JSON instance portion after the schema definition.

    Args:
        prompt (str): Complete prompt including schema and instance request
        model: HuggingFace model instance
        tokenizer: Corresponding tokenizer
        max_steps (int): Maximum generation steps
        top_k (int): Number of candidates to consider per step

    Returns:
        str: The complete prompt followed by generated JSON instance
    """
    # Extract just the instance generation portion (after schema)
    #if '</schema>' in prompt:
     #   generation_prompt = prompt.split('</schema>')[-1].strip()
    #else:
    generation_prompt = prompt
    
    # Initialize PDA and track generation state
    pda = JsonPDA()
    generated_json = "{"
    pda.consume_char('{')  # Start JSON object
    
    print(f"Starting JSON generation from prompt:")

    # Generation loop
    for step in range(max_steps):
        # Get generation context (user request + generated so far)
        context = generation_prompt + generated_json
        candidates = get_top_k_candidates(context, k=top_k, model=model, tokenizer=tokenizer)
        
        if not candidates:
            print("No candidates generated by model")
            break

        print(f"[Step {step}] Candidates: {candidates}")

        # Find first valid continuation
        token_accepted = False
        for token in candidates:
            temp_pda = pda.clone()
            valid = True
            
            # Validate each character in token
            for i, ch in enumerate(token):
                if not temp_pda.consume_char(ch, partial=(i == len(token)-1)):
                    valid = False
                    break
            
            if valid:
                generated_json += token
                pda = temp_pda
                print(f"Accepted token: {repr(token)}")
                token_accepted = True
                
                # Check for completion
                if pda.state == 'END' and not pda.stack:
                    print("Completed JSON object")
                    break
                break
        
        if not token_accepted:
            print("No valid JSON continuation found")
            break
        
        if pda.state == 'END' and not pda.stack:
            break

    # Combine original prompt with generated JSON
    full_output = "\n\n" + generated_json
    print("\n--- Final Output ---")
    print(full_output)
    return full_output


def is_valid_json(s: str) -> bool:
    """Validate if a string is proper JSON"""
    try:
        json.loads(s)
        return True
    except json.JSONDecodeError:
        return False


def evaluate_generation(prompt: str, reference: str, model, tokenizer) -> dict:
    """
    Evaluate JSON generation against a reference
    
    Returns:
        dict: {
            'generated': str,
            'valid': bool,
            'matches_reference': bool,
            'length': int
        }
    """
    generated = generate_with_pda(prompt, model, tokenizer)
    
    # Extract just the JSON portion
    json_start = generated.find('{')
    json_end = generated.rfind('}')
    json_output = generated[json_start:json_end+1] if json_start != -1 and json_end != -1 else ""
    
    return {
        'generated': generated,
        'json_output': json_output,
        'valid': is_valid_json(json_output),
        'matches_reference': json_output.strip() == reference.strip(),
        'length': len(json_output)
    }